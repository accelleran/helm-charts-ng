global:
  # Kubernetes advertise IP
  #   Description:  Supply the Kubernetes Advertise address of your Kubernetes cluster.
  #                 This is used by some services that are exposed via a nodePort.
  #
  #   Value type:   string
  #
  kubeIp: "10.20.20.20"

  # 4G/5G Options
  #   Description:  Depending on the product bought, you may enable or disable the accompanying features.
  #
  #   Value type:   bool
  #
  enable4G: false
  enable5G: true

  # Accelleran License
  #   Description:  In order to use the Accelleran products, you need to have a license from Accelleran.
  #                 Please contact Accelleran to get the license.
  #                 The license file should be saved as a Kubernetes secret. The name of the secret should be
  #                 supplied here under the "secretName" field.
  #                 To create the secret you can use the following command:
  #                 kubectl create secret generic accelleran-license --from-file=license.crt
  #
  # Value type:     enabled: string
  #                 secretName: string
  #
  accelleranLicense:
    enabled: true
    secretName: "accelleran-license"

  nodeSelector: {}

bootstrap:
  create: true
  name: "{{ $.Release.Name }}-bootstrap"
  redis:
    enabled: true
    hostname: ""
    port: 0
  nats:
    enabled: true
    hostname: ""
    port: 0
  kafka:
    enabled: true
    hostname: ""
    port: 0


smo:
  api:
    service:
      enabled: true
      name: ""
      type: ClusterIP
      ports:
        http:
          port: 5000
          targetPort: 5000
          nodePort: null
          protocol: TCP
          appProtocol: http


dashboard:
  enabled: true

  bootstrap:
    create: false
    name: "{{ $.Release.Name }}-bootstrap"

  config:
    # AUTOGENERATED DONT EDIT
    config_id: "production"

    # The namespace where the L3 pod will be running
    defaultServiceNamespace: "default"

    # The namespace where the ORAN-CLUSTERCONTROLLER will be running
    defaultOranNamespace: "default"

    # AUTOGENERATED DONT EDIT
    grafanaURL: "{{ .Values.global.kubeIp }}"

    # The external nodePort used by Grafana, default is 30300
    grafanaPort: "30300"

    # AUTOGENERATED DONT EDIT
    nodeApiURL: "{{ .Values.global.kubeIp }}"
    nodePort: "31315"
    kafkaUrl: "{{ .Release.Name }}-kafka"
    kafkaPort: "9092"
    svcOrchestratorHost: "{{ .Release.Name }}-service-orchestrator"
    svcOrchestratorPort: "80"
    networkStateMonitorHost: "{{ .Release.Name }}-network-state-monitor"
    networkStateMonitorPort: "5000"
    configApiHost: "{{ .Release.Name }}-config-api"
    configApiPort: "80"
    svcMonitorHost: "{{ .Release.Name }}-service-monitor"
    svcMonitorPort: "80"
    pcixAppPodName: "accelleran-drax-pci-010-pci-xapp-api"
    pcixAppPort: "80"
    ksqldbPodName: "{{ .Values.global.kubeIp }}"
    ksqldbPort: "30533"
    LteRadioControllerLabel: "4G-Radio-Controller"
    draxVersionConfigmap: "{{ $.Release.Name }}-version"

    # The IP that is set here must be globally available so that any browser can reach it, so that the front end can communicate to the backend. Preferable, use the publicly exposed IP that can reach the Kubernetes Advertise IP. If only a private Kubernetes Advertise IP is set here, the dRAX Dashbaord will only work when openning a browser on a computer that can reach the private Kuebrnetes Advertise IP.
    apiUrl: "{{ .Values.global.kubeIp }}"
    websocketPort: "31316"
    topologyExpired: 5000
    kafkaTopics:
      - "accelleran.drax.5g.ric.raw.cu_state"
      - "accelleran.drax.5g.ric.raw.ue_measurements"
      - "accelleran.drax.4g.ric.raw.network_state"
      - "accelleran.drax.all.ric.processed.anr"

  service:
    type: NodePort
    ports:
      http:
        port: 5000
        nodePort: 31315
      websocket:
        port: 5001
        nodePort: 31316


network-state-monitor:
  # Enable/disable installation of the Network State Monitor
  enabled: true

  bootstrap:
    create: false
    name: "{{ $.Release.Name }}-bootstrap"


service-monitor:
  # Enable/disable installation of the Service Monitor
  enabled: true

  monitoredNamespaces: "{{ .Release.Namespace }}"

  accelleranLicense:
    enabled: "{{ $.Values.global.accelleranLicense.enabled }}"
    secretName: "{{ $.Values.global.accelleranLicense.secretName }}"


service-orchestrator:
  # Enable/disable installation of the Service Orchestrator
  enabled: true

  kubeIp: "{{ .Values.global.kubeIp }}"

  accelleranLicense:
    enabled: "{{ $.Values.global.accelleranLicense.enabled }}"
    secretName: "{{ $.Values.global.accelleranLicense.secretName }}"


config-api:
  enabled: true

  config:
    default_service_namespace_5g: "default"
    default_oran_namespace_4g: "default"
    service_monitor_host: "{{ .Release.Name }}-service-monitor"
    service_monitor_port: "80"


cell-wrapper:
  enabled: true

  bootstrap:
    create: false
    name: "{{ $.Release.Name }}-bootstrap"

  nats:
    enabled: false
  redis:
    enabled: false


e2-t:
  enabled: true

  numOfE2Nodes: 2

  bootstrap:
    create: false
    name: "{{ $.Release.Name }}-bootstrap"

  nats:
    enabled: false
  redis:
    enabled: false


pm-counters:
  enabled: true


golang-nkafka-5g:
  mode: "5g"

  bootstrap:
    create: false
    name: "{{ $.Release.Name }}-bootstrap"

  config:
    natsConnectionParameters:
      connectTimeout: 5000
      maxReconnects: 120
      reconnectWait: 5000

    natsKafkaTopics:
      override: false
      # defaultKafkaTopic: "accelleran.drax.5g.ric.raw.messages"
      # translations:
      #   - natsSubject: "5G_CUUP_BEACON_INFO"
      #     kafkaTopic: "accelleran.drax.5g.ric.raw.cu_state"
      #   - natsSubject: "PM-REPORT-COUNTERS"
      #     kafkaTopic: "accelleran.drax.5g.ric.raw.pm_counters"
      #   ...

  accelleranLicense:
    enabled: "{{ $.Values.global.accelleranLicense.enabled }}"
    secretName: "{{ $.Values.global.accelleranLicense.secretName }}"


kafka:
  enabled: true

  global:
    storageClass: ""

  nameOverride: ""
  fullnameOverride: ""

  clusterDomain: cluster.local

  extraEnvVars:
    - name: "KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE"
      value: "true"

  listeners:
    client:
      name: CLIENT
      containerPort: 9092
      protocol: PLAINTEXT
    controller:
      name: CONTROLLER
      containerPort: 9093
      protocol: PLAINTEXT
    interbroker:
      name: INTERNAL
      containerPort: 9094
      protocol: PLAINTEXT
    external:
      name: EXTERNAL
      containerPort: 9095
      protocol: PLAINTEXT

  kraft:
    enabled: true

  controller:
    replicaCount: 3
    automountServiceAccountToken: true
    nodeSelector: {}
    persistence:
      size: 1Gi

  broker:
    automountServiceAccountToken: true
    nodeSelector: {}
    persistence:
      size: 1Gi

  provisioning:
    enabled: true
    automountServiceAccountToken: true
    nodeSelector: {}
    replicationFactor: 1
    numPartitions: 1
    topics:
      - name: kafka-cluster-init
        partitions: 1
        replicationFactor: 1

  service:
    type: NodePort

    ports:
      client: 9092
      external: 9095
    nodePorts:
      client: ""
      external: "31090"

  externalAccess:
    enabled: true
    controller:
      service:
        type: "NodePort"
        useHostIPs: true
    broker:
      service:
        type: "NodePort"
        useHostIPs: true
    autoDiscovery:
      enabled: true

  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: true
    annotations: {}
  rbac:
    create: true

  metrics:
    kafka:
      enabled: false
      automountServiceAccountToken: true
      nodeSelector: {}

    jmx:
      enabled: true
      service:
        ports:
          metrics: 5556


nats:
  enabled: true

  global:
    labels:
      drax/technology: 5g
      drax/component-name: nats

  natsBox:
    enabled: false

  promExporter:
    enabled: true
    port: 7777

  service:
    enabled: true
    name: ""

    ports:
      nats:
        enabled: true
      monitor:
        enabled: true

  extraResources:
    - apiVersion: v1
      kind: Service
      metadata:
        name:
          $tplYaml: >
            {{ printf "%s-prom-exporter" (include "nats.fullname" $) | quote }}
      spec:
        selector:
          $tplYaml: |
            {{ include "nats.selectorLabels" $ }}
        ports:
          - protocol: TCP
            port: 7777
            targetPort: 7777


redis:
  enabled: true

  commonLabels:
    drax/technology: 5g
    drax/component-name: redis

  architecture: standalone
  auth:
    enabled: false

  master:
    nodeSelector: {}
    persistence:
      size: 1Gi
  replica:
    nodeSelector: {}


prometheus:
  enabled: true
  nodeSelector: {}

  alertmanager:
    enabled: false

  prometheus-node-exporter:
    enabled: true

  prometheus-pushgateway:
    nodeSelector: {}

  kube-state-metrics:
    nodeSelector: {}

  serviceAccounts:
    kubeStateMetrics:
      create: true
      name: null
    nodeExporter:
      create: true
      name: null
    pushgateway:
      create: true
      name: null
    server:
      create: true
      name: null

  # Retention policy on the Prometheus storage
  server:
    name: server
    persistentVolume:
      enabled: true
      storageClassName: ""
      size: 2Gi
    statefulSet:
      enabled: true
      labels:
        drax/component-name: prometheus
        drax/role: ric

    podLabels:
      drax/component-name: prometheus-server
      drax/role: ric

    retention: 15d
    service:
      labels:
        drax/component-name: prometheus-server
        drax/role: ric
      type: NodePort
      nodePort: 30304

  # adds additional scrape configs to prometheus.yml
  # must be a string so you have to add a | after extraScrapeConfigs:
  extraScrapeConfigs: |
    - job_name: 5gPmCountersXapp
      scrape_interval: 2s
      static_configs:
        - targets:
          - "{{ $.Release.Name }}-pm-counters.{{ $.Release.Namespace }}:8000"
    - job_name: kafkaMonitoringJmx
      static_configs:
        - targets:
          - "{{ $.Release.Name }}-kafka-jmx-metrics.{{ $.Release.Namespace }}:5556"
    - job_name: kafkaMonitoringKminion
      static_configs:
        - targets:
          - "{{ $.Release.Name }}-kminion.{{ $.Release.Namespace }}:8080"
    - job_name: natsMonitoring
      static_configs:
        - targets:
          - "{{ $.Release.Name }}-nats-prom-exporter.{{ $.Release.Namespace }}:7777"


grafana:
  enabled: true
  nodeSelector: {}
  extraLabels:
    drax/role: ric
    drax/component-name: grafana

  persistence:
    type: pvc
    enabled: true
    storageClassName: ""
    size: 1Gi

  service:
    enabled: true
    type: NodePort
    port: 80
    targetPort: 3000
    nodePort: 30300

  # Administrator credentials when not using an existing secret (see below)
  adminUser: accelleran
  adminPassword: accelleran

  grafana.ini:
    auth.basic:
      enabled: false
    auth.anonymous:
      enabled: true
      # Organization name that should be used for unauthenticated users
      org_name: Main Org.
      # Role for unauthenticated users, other valid values are `Editor` and `Admin`
      org_role: Viewer

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - access: proxy
          isDefault: true
          name: Prometheus
          type: prometheus
          url: http://{{ .Release.Name }}-prometheus-server.{{ .Release.Namespace }}:80
          jsonData:
            timeInterval: 2s
        - access: proxy
          isDefault: false
          name: Loki
          type: loki
          url: http://{{ .Release.Name }}-loki.{{ .Release.Namespace }}:3100
        - access: proxy
          isDefault: false
          name: InfluxDB-4G
          type: influxdb
          url: http://{{ .Release.Name }}-influxdb.{{ .Release.Namespace }}:8086
          user: admin
          password: password
          database: db_4G
          basicAuth: true
          basicAuthUser: admin
          basicAuthPassword: password
        - access: proxy
          isDefault: false
          name: InfluxDB-5G
          type: influxdb
          url: http://{{ .Release.Name }}-influxdb.{{ .Release.Namespace }}:8086
          user: admin
          password: password
          database: db_5G
          basicAuth: true
          basicAuthUser: admin
          basicAuthPassword: password

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        # - name: "4g-monitoring-dashboard"
        #   orgId: 1
        #   folder: ""
        #   type: file
        #   disableDeletion: false
        #   editable: true
        #   options:
        #     path: /var/lib/grafana/dashboards/4g-monitoring-dashboard
        # - name: "custom-dashboard"
        #   orgId: 1
        #   folder: ""
        #   type: file
        #   disableDeletion: false
        #   editable: true
        #   options:
        #     path: /var/lib/grafana/dashboards/custom-dashboard
        - name: "loki-log-dashboard"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/loki-log-dashboard
        # - name: "5g-health-dashboard"
        #   orgId: 1
        #   folder: ""
        #   type: file
        #   disableDeletion: false
        #   editable: true
        #   options:
        #     path: /var/lib/grafana/dashboards/5g-health-dashboard
        - name: "5g-monitoring-dashboard"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/5g-monitoring-dashboard
        - name: "5g-cucp-pm-counters"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/5g-cucp-pm-counters
        - name: "5g-cuup-pm-counters"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/5g-cuup-pm-counters
        - name: "kafka-cluster-monitoring"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/kafka-cluster-monitoring
        - name: "nats-dashboard"
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/nats-dashboard

  dashboardsConfigMaps:
    # 4g-monitoring-dashboard: "{{ .Release.Name }}-grafana-4g-monitoring-dashboard"
    # custom-dashboard: "{{ .Release.Name }}-grafana-custom-dashboard"
    loki-log-dashboard: "{{ .Release.Name }}-grafana-loki-log-dashboard"
    # 5g-health-dashboard: "{{ .Release.Name }}-grafana-5g-health-dashboard"
    5g-monitoring-dashboard: "{{ .Release.Name }}-grafana-5g-monitoring-dashboard"
    5g-cucp-pm-counters: "{{ .Release.Name }}-grafana-5g-cucp-pm-counters"
    5g-cuup-pm-counters: "{{ .Release.Name }}-grafana-5g-cuup-pm-counters"
    kafka-cluster-monitoring: "{{ .Release.Name }}-grafana-kafka-cluster-monitoring"
    nats-dashboard: "{{ .Release.Name }}-grafana-nats-dashboard"


loki-stack:
  enabled: true
  promtail:
    config:
      clients:
        - url: http://{{ .Release.Name }}-loki.{{ .Release.Namespace }}:3100/loki/api/v1/push
    nodeSelector: {}
  loki:
    enabled: true
    podLabels:
      drax/role: ric
      drax/component-name: loki
    serviceLabels:
      drax/role: ric
      drax/component-name: loki
    nodeSelector: {}
    config:
      table_manager:
        # Retention polocy for logs in Loki
        retention_deletes_enabled: true
        # Must be a multiple of 168h
        retention_period: 672h


influxdb:
  # Enable/disable installation of InfluxDB
  enabled: true

  podLabels:
    drax/role: ric
    drax/component-name: influxdb

  nodeSelector: {}
  backup:
    nodeSelector: {}

  persistence:
    enabled: true
    # existingClaim: ""
    # storageClass: ""
    accessMode: ReadWriteOnce
    size: 2Gi

  service:
    type: ClusterIP
    nodePorts:
      http: ""

  setDefaultUser:
    enabled: true
    user:
      username: "admin"
      password: "password"

      ## The user name and password are obtained from an existing secret. The expected
      ## keys are `influxdb-user` and `influxdb-password`.
      ## If set, the username and password values above are ignored.
      # existingSecret: influxdb-auth

  initScripts:
    enabled: true
    scripts:
      init_4g.iql: |+
        CREATE DATABASE "db_4G" WITH DURATION 5d REPLICATION 1 NAME "rp_5d_4G"
      init_5g.iql: |+
        CREATE DATABASE "db_5G" WITH DURATION 5d REPLICATION 1 NAME "rp_5d_5G"


vector:
  nameOverride: ""
  fullnameOverride: ""

  role: "Stateless-Aggregator"

  nodeSelector: {}

  image:
    repository: timberio/vector
    pullPolicy: IfNotPresent
    pullSecrets: []
    tag: ""
    sha: ""

  replicas: 1

  secrets:
    generic: {}
      # my_variable: "my-secret-value"
      # datadog_api_key: "api-key"
      # awsAccessKeyId: "access-key"
      # awsSecretAccessKey: "secret-access-key"

  command: []
  args:
    - --config-dir
    - "/etc/vector/"
  env: []
    # - name: VECTOR_LOG
    #   value: "info"
  envFrom: []

  config:
    influx:
      database: db_5G
      namespace: uemeasurement
      hostname: "{{ $.Release.Name }}-influxdb"
      port: 8086
      username: admin
      password: password
      type: influxdb_logs
    kafka:
      hostname: "{{ $.Release.Name }}-kafka"
      port: 9092
      topic: accelleran.drax.5g.ric.raw.ue_measurements
      pmCounterTopic: accelleran.drax.5g.ric.raw.pm_counters

  customConfig:
    sources:
      inUeMeas:
        type: "kafka"
        bootstrap_servers: "{{ tpl $.Values.config.kafka.hostname $ }}:{{ $.Values.config.kafka.port }}"
        group_id: "VECTOR_5G_UE_MEAS"
        topics: ["{{ tpl $.Values.config.kafka.topic $ }}"]

      inUeThroughput:
        type: "kafka"
        bootstrap_servers: "{{ tpl $.Values.config.kafka.hostname $ }}:{{ $.Values.config.kafka.port }}"
        group_id: "VECTOR_5G_UE_THROUGHPUT"
        topics: ["{{ tpl $.Values.config.kafka.pmCounterTopic $ }}"]

    transforms:
      parseJsonUeMeas:
        type: "remap"
        inputs: ["inUeMeas"]
        drop_on_error: true
        source: . |= object!(parse_json!(.message))
      filterUeMeas:
        type: "filter"
        inputs: ["parseJsonUeMeas"]
        condition: "exists(.RrcMeasurementReportResultInfo)"
      parseUeMeas:
        type: "remap"
        inputs: ["filterUeMeas"]
        drop_on_error: true
        source: . |= object!(.RrcMeasurementReportResultInfo)
      luaUeMeas:
        type: "lua"
        inputs: ["parseUeMeas"]
        version: '2'
        hooks:
          process: |-
            function (event, emit)
              m = {}
              m.log = {
                timestamp = os.date("!*t"),
                nrCellId = "",
                ueId = "",
                measurement = {}
              }

              if event.log.RrcMeasurementReportResultInfo.CellInfo ~= nil then
                for f, v in pairs(event.log.RrcMeasurementReportResultInfo.CellInfo) do
                  temp = m
                  temp.log.ueId = event.log.RrcMeasurementReportResultInfo.GnbCuCpUeId
                  temp.log.nrCellId = v.NeighbourCellInfo.NrCgi.NrCellId
                  temp.log.measurement.ssbRsrp = v.NeighbourCellInfo.SsbRsrpResult
                  temp.log.measurement.ssbRsrq = v.NeighbourCellInfo.SsbRsrqResult
                  temp.log.measurement.ssbSinr = v.NeighbourCellInfo.SsbSinrResult
                  emit(temp)
                end
              end

              temp = m
              temp.log.ueId = event.log.RrcMeasurementReportResultInfo.GnbCuCpUeId
              temp.log.nrCellId = event.log.ServingCellInfo.NrCgi.NrCellId
              temp.log.measurement.ssbRsrp = event.log.ServingCellInfo.SsbRsrpResult
              temp.log.measurement.ssbRsrq = event.log.ServingCellInfo.SsbRsrqResult
              temp.log.measurement.ssbSinr = event.log.ServingCellInfo.SsbSinrResult
              emit(temp)
            end

      parseJsonUeThroughput:
        type: "remap"
        inputs: ["inUeThroughput"]
        drop_on_error: true
        source: . |= object!(parse_json!(.message))
      filterUeThroughput:
        type: filter
        inputs: ["parseJsonUeThroughput"]
        condition: exists(.PmReportingCuupCounterData)
      parseUeThroughput:
        inputs: ["filterUeThroughput"]
        type: "remap"
        drop_on_error: true
        source: . |= object!(.PmReportingCuupCounterData)
      filterNatsTopicUeThroughput:
        type: filter
        inputs: ["parseUeThroughput"]
        condition: contains(to_string!(.topic), ".CUUP_COUNTERS_INFO")
      luaParseThr:
        type: lua
        inputs: ["filterNatsTopicUeThroughput"]
        version: '2'
        hooks:
          process: |-
            function (event, emit)
              m = {}
              m.log = {
                timestamp = os.date("!*t"),
                ueId = "",
                measurement = {}
              }

              if event.log.PmReportingCuupCounterData.CounterList ~= nil then
                if event.log.PmReportingCuupCounterData.CounterList.items ~= nil then
                  for k, v in pairs(event.log.PmReportingCuupCounterData.CounterList.items) do
                    temp = m
                    temp.log.ueId = event.log.PmReportingCuupCounterData.CounterList.items[k].InstanceId
                    temp.log.cuupId = event.log.topic:match("[^.]*")
                    if v.CounterId == "DL_GTP_THP" then
                      temp.log.measurement.dlGtpThroughput = 0.0
                      temp.log.measurement.dlGtpThroughput = event.log.PmReportingCuupCounterData.CounterList.items[k].ValueList.items["0"]._val / event.log.PmReportingCuupCounterData.CounterList.items[k].ValueList.items["0"]._key
                      emit(temp)
                    end

                    if v.CounterId == "UL_GTP_THP" then
                      temp.log.measurement.ulGtpThroughput = 0.0
                      temp.log.measurement.ulGtpThroughput = event.log.PmReportingCuupCounterData.CounterList.items[k].ValueList.items["0"]._val / event.log.PmReportingCuupCounterData.CounterList.items[k].ValueList.items["0"]._key
                      emit(temp)
                    end
                  end
                end
              end
            end

    sinks:
      outUeMeas:
        type: "{{ $.Values.config.influx.type }}"
        inputs: ["luaUeMeas"]
        bucket: "vector-bucket"
        database: "{{ $.Values.config.influx.database }}"
        endpoint: "http://{{ tpl $.Values.config.influx.hostname $ }}:{{ $.Values.config.influx.port }}"
        measurement: "{{ $.Values.config.influx.namespace }}.vector"
        tags: ["nrCellId", "ueId"]
        username: "{{ $.Values.config.influx.username }}"
        password: "{{ $.Values.config.influx.password }}"

      outUeThroughput:
        type: "{{ .Values.config.influx.type }}"
        inputs: ["luaParseThr"]
        bucket: "vector-bucket"
        database: "{{ .Values.config.influx.database }}"
        endpoint: "http://{{ tpl .Values.config.influx.hostname . }}:{{ .Values.config.influx.port}}"
        measurement: "{{ .Values.config.influx.namespace }}.vector"
        tags: ["cuupId", "ueId"]
        username: "{{ .Values.config.influx.username }}"
        password: "{{ .Values.config.influx.password }}"

  service:
    enabled: false
    type: "ClusterIP"
    loadBalancerIP: ""

  serviceHeadless:
    enabled: false


kminion:
  enabled: true

  nodeSelector: {}

  kminion:
    # See reference config: https://github.com/cloudhut/kminion/blob/master/docs/reference-config.yaml
    config:
      kafka:
        brokers:
          - "{{ $.Release.Name }}-kafka.{{ $.Release.Namespace }}:9092"
        clientId: "kminion"
        retryInitConnection: true

      minion:
        consumerGroups:
          enabled: true
          scrapeMode: offsetsTopic
          granularity: partition
          allowedGroups: ["*"]
          ignoredGroups: []
        topics:
          granularity: partition
          allowedTopics: []
          ignoredTopics: []
          infoMetric:
            configKeys: ["cleanup.policy"]
        logDirs:
          enabled: false

      exporter:
        namespace: "kminion"
        host: ""
        port: 8080

      logger:
        level: info

  tests:
    enabled: false
